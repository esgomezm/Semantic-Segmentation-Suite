{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluate.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNTYR8B/zHtW9TmfTh+0U1w"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"v2Qo9jZZUZOG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1598461365097,"user_tz":-120,"elapsed":20940,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"1cbdb7c3-4c2c-4394-e157-d2d8640a9cc7"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g26hKd6HUf2N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1598461367026,"user_tz":-120,"elapsed":22859,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"a0519be8-dccf-4a77-e02b-f577318b14f0"},"source":["import os\n","os.chdir('/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/PHCellTrackSeg')\n","!ls\n","import pandas as pd\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["drive_data_management.ipynb  Mask_merge.ipynb  res_track.txt\n","im2RGB.py\t\t     README.md\t       stack2im.py\n","man_track.txt\t\t     requirements.txt  utils\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"elqK0LIJVgZ4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"status":"ok","timestamp":1598461382121,"user_tz":-120,"elapsed":37947,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"71a1a05b-e84b-4cf7-a755-180d26eb22c5"},"source":["!pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting opencv-python-headless\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2a/496e06fd289c01dc21b11970be1261c87ce1cc22d5340c14b516160822a7/opencv_python_headless-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (36.6MB)\n","\u001b[K     |████████████████████████████████| 36.6MB 112kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.16.2)\n","Collecting SimpleITK\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d8/53338c34f71020725ffb3557846c80af96c29c03bc883551a2565aa68a7c/SimpleITK-1.2.4-cp36-cp36m-manylinux1_x86_64.whl (42.5MB)\n","\u001b[K     |████████████████████████████████| 42.5MB 100kB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 4)) (2.4)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 4)) (7.0.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 4)) (3.2.2)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 4)) (2.4.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 4)) (4.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 4)) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 4)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 4)) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 4)) (1.15.0)\n","Installing collected packages: opencv-python-headless, SimpleITK\n","Successfully installed SimpleITK-1.2.4 opencv-python-headless-4.4.0.42\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nbwlGqUfU60A","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598461383828,"user_tz":-120,"elapsed":25613,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}}},"source":["import sys\n","from utils.build_processed_videos import get_accuracy_measures_videos, build_videos, jaccard_index, dice_coeff\n","import utils.build_processed_videos\n","import os\n","import SimpleITK as sitk\n","import numpy as np\n","from scipy.spatial.distance import directed_hausdorff\n","import sys\n","from skimage.measure import regionprops\n","from scipy import ndimage"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uEGSmxlVO1E","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598461383829,"user_tz":-120,"elapsed":25269,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}}},"source":["def jaccard_index (y_pred, y_true):\n","    intersection = y_true*y_pred\n","    intersection = intersection.astype(np.float)\n","    union = y_true + y_pred - intersection\n","    union = union.astype(np.float)\n","    if np.sum(union) == 0.0:\n","        return 1.0\n","    else:\n","        return np.sum(intersection)/np.sum(union)\n","\n","def dice_coeff (y_pred, y_true):\n","    intersection = y_true*y_pred\n","    intersection = intersection.astype(np.float)\n","    union = y_true + y_pred\n","    union = union.astype(np.float)\n","    if np.sum(union) == 0.0:\n","        return 1.0\n","    else:\n","        return 2*np.sum(intersection)/np.sum(union)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcvkncUof8Ud","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598461384880,"user_tz":-120,"elapsed":1049,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}}},"source":["def get_accuracy_measures_validation(PATH2VIDEOS, PATH2VALSCORE, PATH2STORE, EXPERIMENT):\n","    '''\n","    PATH2RESULTS: path where the results are placed.\n","    PATH2GT: path to the ground truth data.\n","    FILE2STORE: path to a file where the results will be saved.\n","    PATH2VIDEOS: path to a txt file in which each row contains the name of the video and the frame that belongs to that\n","                video:\n","        Labels row      Videos ; Frames\n","                        video 1; raw_001.tif\\n\n","                        video 1; raw_002.tif\\n\n","                        ...\n","                        video 1; raw_032.tif\\n\n","                        video 2; raw_033.tif\\n\n","    '''\n","\n","    # FILES = os.listdir(PATH2VIDEOS)\n","\n","    # Read the file containing the relation between the initial videos and the individual frames.\n","    files = [x for x in open(PATH2VIDEOS, \"r\")]\n","    files = files[1:]  # First row contains labels\n","    file_relation = [[x.split(';')[0], x.split(';')[1][:-1]] for x in files]\n","\n","    results = pd.read_csv(PATH2VALSCORE,sep=',',header=0)\n","    \n","#    Need to change the name of the frames to adapt to our input (diferentiate between gt and pred)\n","\n","    COUNT = 1\n","    if not os.path.exists(PATH2STORE):\n","        os.makedirs(PATH2STORE)\n","\n","    while COUNT <= len(file_relation):\n","        # Get the name of the original videos and the number of frames that it contains\n","        file_name = file_relation[COUNT][0]  # video name\n","        # Calculate how many frames you need to process (it is said in the name of the video)\n","        start_time, end_time = file_name.split('_')[-1].split('-')\n","        start_time = np.int(start_time)\n","        end_time = np.int(end_time)\n","        # TODO: check how many frames belong to the same video and reconstruct them\n","        sys.stdout.write(\"\\rProcessing video {0}:\\n\".format(file_name))\n","        \n","        Cell_validation=np.zeros(len(range(end_time - start_time +1)))\n","        precision=np.zeros(len(range(end_time-start_time+1)))\n","        fp = np.zeros(len(range(end_time-start_time+1)))\n","        \n","        for i in range(end_time - start_time + 1):\n","            # Load the single frame\n","            frame_name = os.path.join(PATH2VIDEOS, 'raw_{0:0>3}_pred.tif'.format(int(COUNT + i)))\n","\n","            # Calculate the mean accuracy measures for each video\n","            Cell_validation [i] = results[' Cell'][COUNT+i]\n","            precision [i] = results[' precision'][COUNT+i]\n","            fp[i] = (Cell_validation [i]/ precision [i])-Cell_validation [i]\n","            progress = (i + 1) / (end_time - start_time + 1)\n","            \n","\n","            text = \"\\r[{0}] {1}%\".format(\"-\" * (i + 1) + \" \" * (end_time - start_time - i), progress * 100)\n","            sys.stdout.write(text)\n","            sys.stdout.flush()\n","        sys.stdout.write(\"\\n\")  # this ends the progress bar\n","        COUNT = COUNT + i + 1\n","        Cell_vali = np.mean(Cell_validation)\n","        prec = np.mean(precision)\n","        fpt = np.mean(fp)\n","        \n","        # Store the measures for each video in a csv file\n","        if os.path.exists(os.path.join(PATH2STORE, 'video_accuracies_val' + EXPERIMENT + '.csv')):\n","            with open(os.path.join(PATH2STORE, 'video_accuracies_val' + EXPERIMENT +'.csv'), mode='a') as file_:\n","                file_.write(file_name + \";{};{};{}\".format(Cell_vali, prec, fpt))\n","                file_.write(\"\\n\")\n","        else:\n","            fields = \"video; Cell_validation; Precission; FP;\"\n","            with open(os.path.join(PATH2STORE, 'video_accuracies_val' + EXPERIMENT +'.csv'), 'w') as file_:\n","                file_.write(fields)\n","                file_.write(\"\\n\")\n","                file_.write(file_name + \";{};{};{}\".format(Cell_vali,prec,fpt))\n","                file_.write(\"\\n\")\n","                \n","    print(\"All videos have been evaluated\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ovx3_3Nni1rj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598461384881,"user_tz":-120,"elapsed":1045,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}}},"source":["PATH2VIDEOS = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/clean_data/data4training/test/videos2im_relation.csv'\n","PATH2VALSCORE = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/RESULTS-SERVER/checkpoints/constant_lr_tiramisu103_v01/2900/val_scores.csv'\n","PATH2STORE = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/eval_data_server/validation/'\n","EXPERIMENT = '_Model5'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"4O80XLaQkXv4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1598461423837,"user_tz":-120,"elapsed":38864,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"56cc9043-4b8b-49c1-c825-7fa073f8661e"},"source":["get_accuracy_measures_validation(PATH2VIDEOS, PATH2VALSCORE, PATH2STORE, EXPERIMENT)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Processing video 5000_6-15-14_1_xy004_255-354:\n","[----------------------------------------------------------------------------------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-8_6-15-14_1_xy033_181-183:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy015_002-004:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy001_240-242:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy008_078-080:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy005_157-171:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_6-1-14_1001_xy28_330-332:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_267-281:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_370-385:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_090-104:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_340-355:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_040-064:\n","[-------------------------] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_289-312:\n","[------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_452-467:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_306-325:\n","[--------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_001-049:\n","[------------------------------------------------ ] 97.95918367346938%"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-a32c6b4bee06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_accuracy_measures_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH2VIDEOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH2VALSCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH2STORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-f534b220c3a0>\u001b[0m in \u001b[0;36mget_accuracy_measures_validation\u001b[0;34m(PATH2VIDEOS, PATH2VALSCORE, PATH2STORE, EXPERIMENT)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Calculate the mean accuracy measures for each video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mCell_validation\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' Cell'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUNT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mprecision\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUNT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCell_validation\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mCell_validation\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 326"]}]},{"cell_type":"markdown","metadata":{"id":"3y0oKtx93wZL","colab_type":"text"},"source":["Get accuaracy measures"]},{"cell_type":"code","metadata":{"id":"IHWCDijoVugN","colab_type":"code","colab":{}},"source":["PATH2RESULTS = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/Semantic-Segmentation-Suite/constante_tiramisu/2980/'\n","PATH2GT = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/Semantic-Segmentation-Suite/constante_tiramisu/2980/'\n","PATH2STORE = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/eval_data_server/constante_tiramisu/2980/'\n","PATH2VIDEOS = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/clean_data/data4training/test/videos2im_relation.csv'\n","EXPERIMENT = '2980'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3c5fse9dw_8U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"ok","timestamp":1598437225779,"user_tz":-120,"elapsed":284128,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"d9ca6e01-f670-4c74-864a-969516e72dde"},"source":["get_accuracy_measures_videos(PATH2RESULTS, PATH2GT, PATH2STORE, PATH2VIDEOS, EXPERIMENT)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing video 5000_6-15-14_1_xy004_255-354:\n","[----------------------------------------------------------------------------------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-8_6-15-14_1_xy033_181-183:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy015_002-004:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy001_240-242:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy008_078-080:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy005_157-171:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_6-1-14_1001_xy28_330-332:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_267-281:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_370-385:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_090-104:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_340-355:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_040-064:\n","[-------------------------] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_289-312:\n","[------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_452-467:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_306-325:\n","[--------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_001-049:\n","[-------------------------------------------------] 100.0%\n","All videos have been evaluated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oS2Oae5uVZKY","colab_type":"code","colab":{}},"source":["def get_accuracy_measures_videos(PATH2RESULTS, PATH2GT, PATH2STORE, PATH2VIDEOS, EXPERIMENT):\n","    '''\n","    PATH2RESULTS: path where the results are placed.\n","    PATH2GT: path to the ground truth data.\n","    FILE2STORE: path to a file where the results will be saved.\n","    PATH2VIDEOS: path to a txt file in which each row contains the name of the video and the frame that belongs to that\n","                video:\n","        Labels row      Videos ; Frames\n","                        video 1; raw_001.tif\\n\n","                        video 1; raw_002.tif\\n\n","                        ...\n","                        video 1; raw_032.tif\\n\n","                        video 2; raw_033.tif\\n\n","    '''\n","\n","    # FILES = os.listdir(PATH2VIDEOS)\n","\n","    # Read the file containing the relation between the initial videos and the individual frames.\n","    files = [x for x in open(PATH2VIDEOS, \"r\")]\n","    files = files[1:]  # First row contains labels\n","    file_relation = [[x.split(';')[0], x.split(';')[1][:-1]] for x in files]\n","    \n","#    Need to change the name of the frames to adapt to our input (diferentiate between gt and pred)\n","\n","    COUNT = 1\n","    if not os.path.exists(PATH2STORE):\n","        os.makedirs(PATH2STORE)\n","\n","    while COUNT <= len(file_relation):\n","        # Get the name of the original videos and the number of frames that it contains\n","        file_name = file_relation[COUNT][0]  # video name\n","        # Calculate how many frames you need to process (it is said in the name of the video)\n","        start_time, end_time = file_name.split('_')[-1].split('-')\n","        start_time = np.int(start_time)\n","        end_time = np.int(end_time)\n","        # TODO: check how many frames belong to the same video and reconstruct them\n","        sys.stdout.write(\"\\rProcessing video {0}:\\n\".format(file_name))\n","        HM=np.zeros(len(range(end_time - start_time +1)))\n","        JAC = np.zeros(len(range(end_time - start_time +1)))\n","        DICE = np.zeros(len(range(end_time - start_time +1)))\n","        DICE_cell_all=np.zeros(len(range(end_time - start_time +1)))\n","        JAC_cell_all=np.zeros(len(range(end_time - start_time +1)))\n","        HM_cell_all = np.zeros(len(range(end_time - start_time +1)))\n","        for i in range(end_time - start_time + 1):\n","            # Load the single frame\n","            frame_name = os.path.join(PATH2RESULTS, 'raw_{0:0>3}_pred.tif'.format(int(COUNT + i)))\n","            frame = sitk.ReadImage(frame_name)\n","            frame = sitk.GetArrayFromImage(frame)\n","            gt_name = os.path.join(PATH2GT, 'raw_{0:0>3}_gt.tif'.format(int(COUNT + i)))\n","            gt = sitk.ReadImage(gt_name)\n","            gt = sitk.GetArrayFromImage(gt)\n","            gt[gt > 0] = 1\n","            frame[frame < 0.5] = 0\n","            frame[frame > 0] = 1\n","            # Calculate the mean accuracy measures for each video\n","            HM[i]=directed_hausdorff(frame,gt)[0]\n","            JAC[i] = jaccard_index(frame.flatten(), gt.flatten())\n","            DICE [i] = dice_coeff(frame.flatten(), gt.flatten())\n","            DICE_cell_all[i], JAC_cell_all[i], HM_cell_all[i] = get_accuracy_cells(gt,frame)\n","            \n","            # if i == 0:\n","            #     HM[i] = directed_hausdorff(frame, gt)[0]\n","            #     # HM_total = HM\n","\n","            #     JAC = jaccard_index(frame.flatten(), gt.flatten())\n","            #     # JAC_total = JAC\n","\n","            #     DICE = dice_coeff(frame.flatten(), gt.flatten())\n","            #     # DICE_total = DICE\n","\n","            #     DICE_cell_all, JAC_cell_all, HM_cell_all = get_accuracy_cells(gt,frame)\n","            # else:\n","            #     HM = (HM*i + directed_hausdorff(frame, gt)[0])/(i+1)\n","            #     # HM_total = directed_hausdorff(frame, gt)[0]+HM_total\n","\n","            #     JAC = (JAC*i + jaccard_index(frame.flatten(), gt.flatten()))/(i+1)\n","            #     # JAC_total = jaccard_index(frame.flatten(), gt.flatten()) + JAC_total\n","\n","            #     DICE = (DICE*i + dice_coeff(frame.flatten(), gt.flatten()))/(i+1)\n","            #     # DICE_total = dice_coeff(frame.flatten(), gt.flatten()) + DICE_total\n","                \n","                \n","            #     DICE_cell, JAC_cell, HM_cell = get_accuracy_cells(gt,frame)\n","            #     DICE_cell_all = (DICE_cell_all*i + DICE_cell)/(i+1)\n","            #     JAC_cell_all = (JAC_cell_all*i + JAC_cell)/(i+1)\n","            #     HM_cell_all = (HM_cell_all*i + HM_cell)/(i+1)\n","\n","\n","            progress = (i + 1) / (end_time - start_time + 1)\n","            text = \"\\r[{0}] {1}%\".format(\"-\" * (i + 1) + \" \" * (end_time - start_time - i), progress * 100)\n","            sys.stdout.write(text)\n","            sys.stdout.flush()\n","        sys.stdout.write(\"\\n\")  # this ends the progress bar\n","        COUNT = COUNT + i + 1\n","        HM_mean = np.mean(HM)\n","        HM_dev = np.std(HM)\n","        JAC_mean = np.mean(JAC)\n","        JAC_dev = np.std(JAC)\n","        DICE_mean = np.mean(DICE)\n","        DICE_dev = np.std(DICE)\n","        DICE_cell_all_mean = np.mean(DICE_cell_all)\n","        DICE_cell_all_dev = np.std(DICE_cell_all)\n","        JAC_cell_all_mean = np.mean(JAC_cell_all)\n","        JAC_cell_all_dev = np.std(JAC_cell_all)\n","        HM_cell_all_mean = np.mean(HM_cell_all)\n","        HM_cell_all_dev = np.std(HM_cell_all)\n","                \n","        # Store the measures for each video in a csv file\n","        if os.path.exists(os.path.join(PATH2STORE, 'video_accuracies_' + EXPERIMENT + '.csv')):\n","            with open(os.path.join(PATH2STORE, 'video_accuracies_' + EXPERIMENT +'.csv'), mode='a') as file_:\n","                file_.write(file_name + \";{};{};{};{};{};{};{};{};{};{};{};{}\".format(HM_mean, HM_dev, JAC_mean, JAC_dev, DICE_mean, DICE_dev, HM_cell_all_mean, HM_cell_all_dev, JAC_cell_all_mean, JAC_cell_all_dev, DICE_cell_all_mean, DICE_cell_all_dev))\n","                # file_.write(file_name + \";{};{};{}\".format(HM, JAC, DICE))\n","                # for i in range(total_cells):\n","                #     file_.write( \";{};{};{};\".format(HM_cell[i], JAC_cell[i] , DICE_cell[i]))\n","                file_.write(\"\\n\")\n","        else:\n","            fields = \"video; Hausdorff-distance; Haus_dev; Jaccard index; JAC_ dev; Dice coeffficient; DICE_dev; Mean Hausdorff-distance; Mean_Haus_dev; Mean Jaccard index; Mean_JAC_dev; Mean Dice coefficient; Mean_DICE_dev\"\n","            # fields = \"video; Hausdorff-distance; Jaccard index; Dice coeffficient\"\n","            with open(os.path.join(PATH2STORE, 'video_accuracies_' + EXPERIMENT +'.csv'), 'w') as file_:\n","                file_.write(fields)\n","                file_.write(\"\\n\")\n","                file_.write(file_name + \";{};{};{};{};{};{};{};{};{};{};{};{}\".format(HM_mean, HM_dev, JAC_mean, JAC_dev, DICE_mean, DICE_dev, HM_cell_all_mean, HM_cell_all_dev, JAC_cell_all_mean, JAC_cell_all_dev, DICE_cell_all_mean, DICE_cell_all_dev))\n","                # file_.write(file_name + \";{};{};{}\".format(HM, JAC, DICE))\n","                # for i in range(total_cells):\n","                #     file_.write( \";{};{};{};\".format(HM_cell[i], JAC_cell[i] , DICE_cell[i]))\n","                file_.write(\"\\n\")\n","                \n","    print(\"All videos have been evaluated\")\n","    \n","  \n","    \n","def get_accuracy_cells (gt, pred):\n","   \n","    gt_labels, gt_nlabels = ndimage.measurements.label(gt)\n","    pred_labels, pred_nlabels = ndimage.measurements.label(pred)\n","\n","    HM= np.zeros(pred_nlabels)\n","    JAC = np.zeros(pred_nlabels)\n","    DICE= np.zeros(pred_nlabels)\n","    count=0\n","    val_nlabels = np.arange(gt_nlabels+1)\n","    \n","\n","#Esto cuenta con respecto de las células que coinciden con el ground truth, pero como el jac y el dice están hechos a la length de la predicción, si hay células que no estan, el jac y el dice ya son cero de por si\n","    for j in val_nlabels[1:]:\n","      gt_label = gt_labels == int(j)\n","      gt_label [gt_label>0.5] =1\n","      overlap = np.multiply(gt_label, pred_labels)\n","      if np.sum(overlap)>0:\n","            candidates = np.unique(overlap) # at least two values: 0 and prediction_label\n","            overlap_sum = 0 #creo que inutil\n","            for i in candidates[1:]:\n","                # label2eval = 0\n","                # overlap_sum = np.sum(overlap == int(i))\n","                # if np.sum(overlap == int(i)) > overlap_sum and np.sum(overlap == int(i)) >= 0.5*np.sum(gt_label) :\n","                if np.sum(overlap == int(i)) >= 0.5*np.sum(gt_label) :\n","                    label2eval = int(i)\n","                    pred2eval =[]\n","                    pred2eval=pred_labels==int(i)\n","                    pred2eval=1*pred2eval\n","                    HM [count] = directed_hausdorff(pred2eval, gt_label)[0]\n","                    JAC [count] = jaccard_index(pred2eval.flatten(), gt_label.flatten())\n","                    DICE [count] = dice_coeff(pred2eval.flatten(), gt_label.flatten())\n","                    pred_labels[pred_labels == int(i)] = 0 # creo que inutil \n","                    count +=1\n","\n","    HM = np.sum(HM)/pred_nlabels\n","    JAC = np.sum(JAC)/pred_nlabels\n","    DICE = np.sum(DICE)/pred_nlabels\n","   \n","    return DICE, JAC, HM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSUve3zws3EK","colab_type":"code","colab":{}},"source":["INPUTPATH = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/Seg_github/Semantic-Segmentation-Suite/Test/'\n","OUTPUTPATH = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/intento/pred/'\n","PATH2VIDEOS = '/content/gdrive/My Drive/TFG/TFG MARINA CALZADA/clean_data/data4training/test/videos2im_relation.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQFHfNRwspuK","colab_type":"code","colab":{}},"source":["def build_videos(INPUTPATH,OUTPUTPATH,PATH2VIDEOS):\n","    '''\n","    INPUTPATH: path where the single frames are placed.\n","    OUTPUTPATH: pathe where the reconstructed videos will be stored.\n","    PATH2VIDEOS: path to a txt file in which each row contains the name of the video and the frame that belongs to that\n","                video:\n","        Labels row      Videos ; Frames\n","                        video 1; raw_001.tif\\n\n","                        video 1; raw_002.tif\\n\n","                        ...\n","                        video 1; raw_032.tif\\n\n","                        video 2; raw_033.tif\\n\n","    '''\n","    files = [x for x in open(PATH2VIDEOS, \"r\")]\n","    files = files[1:]  # First row contains labels\n","    file_relation = [[x.split(';')[0], x.split(';')[1][:-1]] for x in files]\n","\n","    if not os.path.exists(OUTPUTPATH):\n","        os.makedirs(OUTPUTPATH)\n","\n","    COUNT = 1\n","    while COUNT <= len(file_relation):\n","        # Get the name of the original videos and the number of frames that it contains\n","        file_name = file_relation[COUNT][0] # video name\n","        # Calculate how many frames you need to process (it is said in the name of the video)\n","        start_time, end_time = file_name.split('_')[-1].split('-')\n","        start_time = np.int(start_time)\n","        end_time = np.int(end_time)\n","        sys.stdout.write(\"\\rProcessing video {0}:\\n\".format(file_name))\n","\n","        for i in range(end_time-start_time+1):\n","            # Load the single frame\n","            frame_name = os.path.join(INPUTPATH, 'raw_{0:0>3}_pred.tif'.format(int(COUNT+i)))\n","            frame = sitk.ReadImage(frame_name)\n","            frame = sitk.GetArrayFromImage(frame)\n","            if i == 0:\n","                video = frame.reshape((1,frame.shape[0], frame.shape[1]))\n","            else:\n","                video = np.concatenate((video,frame.reshape((1,frame.shape[0], frame.shape[1]))), axis=0)\n","            progress = (i+1)/(end_time-start_time+1)\n","            text = \"\\r[{0}] {1}%\".format(\"-\" * (i+1) + \" \" * (end_time-start_time-i), progress * 100)\n","            sys.stdout.write(text)\n","            sys.stdout.flush()\n","        COUNT = COUNT + i + 1\n","        sitk.WriteImage(sitk.GetImageFromArray(video.astype(np.float32)), os.path.join(OUTPUTPATH, file_name + '.tif'))\n","        sys.stdout.write(\"\\n\")  # this ends the progress bar\n","    print(\"All videos have been reconstructed\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AC6dZq0xtDOT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"ok","timestamp":1590442575415,"user_tz":-120,"elapsed":120811,"user":{"displayName":"MARINA CALZADA GARCIA","photoUrl":"","userId":"05425390168260934322"}},"outputId":"3e190278-5b1f-4ba9-ed58-1d6bc9c5bb80"},"source":["build_videos(INPUTPATH,OUTPUTPATH,PATH2VIDEOS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing video 5000_6-15-14_1_xy004_255-354:\n","[----------------------------------------------------------------------------------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-8_6-15-14_1_xy033_181-183:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy015_002-004:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy001_240-242:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy008_078-080:\n","[---] 100.0%\n","Processing video 5000_7-1-14_001_xy005_157-171:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_6-1-14_1001_xy28_330-332:\n","[---] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_267-281:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_370-385:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_090-104:\n","[---------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_340-355:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy30_040-064:\n","[-------------------------] 100.0%\n","Processing video 10000_11-20-13_1003_xy018_289-312:\n","[------------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_452-467:\n","[----------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_306-325:\n","[--------------------] 100.0%\n","Processing video Videos and corresponding protrusion analysis_IL-6_12-5-12_1_xy27_001-049:\n","[-------------------------------------------------] 100.0%\n","All videos have been reconstructed\n"],"name":"stdout"}]}]}